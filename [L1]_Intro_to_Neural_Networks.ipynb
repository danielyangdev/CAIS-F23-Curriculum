{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielyangdev/CAIS-F23-Curriculum/blob/main/%5BL1%5D_Intro_to_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKEe7MzvveKD"
      },
      "source": [
        "# **Lesson 1 Notebook: Neural Networks**\n",
        "**Notebook Objective:**\n",
        "In this notebook, you'll experiment with defining your own neural network architecture to classify handwritten digits of the MNIST dataset. You'll work with loading and viewing the data, as well as initializing and training a neural network from scratch. Let's get started!\n",
        "**Useful Links:**\n",
        "- [A Comprehensive Guide to Data Loading](https://blog.paperspace.com/dataloaders-abstractions-pytorch/): covers built-in datasets within Pytorch and the *DataLoader* class\n",
        "* [Pytorch Neural Network(NN Module)](https://pytorch.org/docs/stable/nn.html)\n",
        "* [Torch Optimizers](https://pytorch.org/docs/stable/optim.html)\n",
        "* [Activation Functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
        "* [Cost/Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBjtJ6byveKF"
      },
      "source": [
        "## **Loading the Data**\n",
        "\n",
        "We'll be using the pre-labeled [**MNIST dataset**](https://en.wikipedia.org/wiki/MNIST_database), which contains 70,000 grayscale images of handwritten digits, along with their corresponding digit labels.\n",
        "We will use 60,000 images for training, and the remaining 10,000 for evaluation. We can easily access this dataset using `torchvision.datasets.MNIST()`, and define our train and test loaders using `torch.utils.data.DataLoader`. Hint: we want to shuffle the training data in order to maximize similarity of different batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tdNnxIrveKF",
        "nbpresent": {
          "id": "aa21eec0-d9e9-4575-8cec-408d8f044b20"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device config: use cuda if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Define batch size for data loader: number of samples processed before the model is updated\n",
        "batch_size = #TODO--------- up to you but conventionally power of 2   # Number of samples processed before the model is updated\n",
        "\n",
        "# Load and Initalize MNIST training and test data from PyTorch\n",
        "  #Variable Defs:\n",
        "    #root= string: directory where the database is stored.\n",
        "    #train= boolean: defines if training set\n",
        "    #download= boolean: if want to download dataset\n",
        "    #transform= function: to specify what transformation is taking place\n",
        "\n",
        "# Load MNIST training and test data from PyTorch\n",
        "train_data = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# Create data loader\n",
        "train_loader = #TODO: use torch.utils.data.DataLoader and define the dataset, batch_size, and shuffle parameters\n",
        "test_loader = #TODO: use torch.utils.data.DataLoader and define the dataset, batch_size, and shuffle parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDZx1YN5veKI",
        "nbpresent": {
          "id": "b04508f5-7038-4d57-a3ad-a8e1dd3e172e"
        },
        "scrolled": true
      },
      "source": [
        "# Data Exploration\n",
        "\n",
        "# How many training and testing examples\n",
        "#TODO-------check the length of the MNIST test set using the Python len function to\n",
        "#       get the number of items to make sure it matches what we expect.\n",
        "print(\"Number of training examples:\", _______))\n",
        "print(\"Number of testing examples:\", _______))\n",
        "\n",
        "# What features are given for each example?\n",
        "examples = iter(train_loader)\n",
        "samples, labels = examples.next()\n",
        "print(\"Sample shape: \", samples.shape)\n",
        "print(\"Label shape: \", labels.shape)\n",
        "\n",
        "# What do our labels look like?\n",
        "print(\"Labels: \", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjBCbG4KveKK"
      },
      "source": [
        "# Data Visualization: use matplotlib to see some given images of handwritten digits\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Change this number and re-run the cell to see different image samples!\n",
        "sample_num = ___________\n",
        "\n",
        "plt.imshow(samples[sample_num][0], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D79_gwvlveKU"
      },
      "source": [
        "## **Defining the Neural Network**\n",
        "\n",
        "1. **Initialize the network**: We'll first define our own NN Model with specific layers and activation functions that will be used during training. We will decide the \"settings\" of our NN, also known as the **hyperparameters**, such as the number of layers, number of neurons per layer, etc. We will use `nn.Linear(input_size, output_size)` for each of our layers, and **ReLU** as our intermediate activation functions.\n",
        "- [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "- [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
        "2. **Define forward method**: Given that we're using PyTorch, we need to define our own method for forwarding input data through the network. All you need to do is to pass through the input through each layer and activation function that you've defined in step 1! :)\n",
        "2. **Compile** the network to get ready for training. This tells the network what cost/loss function to use (\"cost\" and \"loss\" are used interchangeably), and what type of gradient descent, or optimizer, to use. Since we are outputting 10 different classes of digits, we want to use **categorical crossentropy loss**.\n",
        "- [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "- [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
        "3. **Training** the network, using the set of training images. This actually feeds the training data into the network, and uses gradient descent and backpropagation to adjust the network's weights in order to minimize the cost function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dhgvgeHveKU",
        "nbpresent": {
          "id": "81b9fc81-511c-4d2d-be23-883d0009bc74"
        }
      },
      "source": [
        "# TODO: Define function to create our own neural network\n",
        "\n",
        "# Parameters\n",
        "input_size = #TODO    # Hint: image size is 28x28, and we want to flatten the image into a ?? by 1 vector\n",
        "num_classes = #TODO   # Hint: our inputs include 0-9\n",
        "num_epochs = #TODO    # Number of times we loop through the entire training dataset\n",
        "\n",
        "class NN(nn.Module):\n",
        "  ############ YOUR CODE STARTS HERE ############\n",
        "  # 1. Initialize our own NN model\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(NN, self).__init__()\n",
        "    # Use ReLU activation function\n",
        "    self.relu = #TODO-----------\n",
        "    # Input layer\n",
        "    self.input_layer = #TODO------------\n",
        "    # Hidden layers: use at least 1 hidden layer!\n",
        "    self.hidden1 = #TODO-----------\n",
        "    # Output layer\n",
        "    self.output_layer = #TODO------------\n",
        "\n",
        "  # 2. Define method for forwarding input data\n",
        "  def forward(self, sample):\n",
        "\n",
        "    out = #TODO: pass input into input layer\n",
        "    out = #TODO: activation function\n",
        "\n",
        "    out = #TODO: pass into all hidden layer(s)\n",
        "    out = #TODO: activation function\n",
        "\n",
        "    out = #TODO: forward to output layer\n",
        "    return out\n",
        "  ############ YOUR CODE ENDS HERE ################\n",
        "# Initialize our model by passing in our hyperparameters defined previously\n",
        "nn_model = NN(input_size, num_classes)\n",
        "print(\"My NN Model: \", nn_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4M0QIT3veKX",
        "nbpresent": {
          "id": "bf842b6a-20db-42f4-9333-b1e30857e725"
        }
      },
      "source": [
        "# 3 Network Compilation: Loss function & Optimizer\n",
        "\n",
        "loss_function = #TODO: categorical cross entropy loss\n",
        "optimizer = #TODO: define an optimizer to use with torch.optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaP6NX1SveKZ"
      },
      "source": [
        "# 4 -- Network Training\n",
        "\n",
        "# TODO Fit the model to the data. Number of epochs: ___ (up to you). Batch size: ___ (up to you, conventionally a power of 2)\n",
        "# ** Values for Epoch and Batch are defined in previous cells\n",
        "total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # Reshape our images from 2D(28x28) to 1D(784)\n",
        "    images = # TODO ____________.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Call functions we've previously defined to perform forward pass & calculate loss\n",
        "    output = #TODO-----------\n",
        "    loss = #TODO-----------\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.________ # TODO: Empty accumulated values in the gradient attribute\n",
        "    loss.________ # TODO: Backpropagation\n",
        "    optimizer.________  # TODO: Update parameters\n",
        "\n",
        "    # Print out training process\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{total_steps}, loss = {loss.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7426fdCMveKb"
      },
      "source": [
        "## **Evaluating the Model**\n",
        "Here we will see how our model performs on unseen test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFftjUuNveKc",
        "nbpresent": {
          "id": "0864543a-2392-4ae5-bbdd-375e934184f4"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89241da1-6548-4fcb-fe62-a857ed904097"
      },
      "source": [
        "# 4 -- Evaluate on the test data!\n",
        "\n",
        "# Reduce memory consumption by not letting PyTorch to calculate gradient since no backword passing is involved in testing\n",
        "with torch.no_grad():\n",
        "  accuracy_arr = []\n",
        "  loss_arr = []\n",
        "  correct_predict_num = 0\n",
        "  sample_num = 0  # Total num of test samples in current loop\n",
        "  # Loop all batches in the test dataset\n",
        "  for images, labels in test_loader:\n",
        "    # Reshape inputs\n",
        "    images = images.reshape(______).to(device) #Calculate dimensions to reshape\n",
        "    labels = labels.to(device)\n",
        "    # Use our own model to make prediction\n",
        "    output = nn_model(images)\n",
        "\n",
        "    # Get Prediction and loss\n",
        "    _, predictions = torch.max(output, 1)\n",
        "    sample_num += labels.shape[0]\n",
        "    correct_predict_num += (predictions == labels).sum().item()\n",
        "\n",
        "    # Print out loss and accuracy\n",
        "    loss = loss_function(output, labels)\n",
        "    accuracy = #TODO: calculate accuracy\n",
        "    accuracy_arr.append(accuracy)\n",
        "    loss_arr.append(loss)\n",
        "\n",
        "print(f'Average Accuracy: {sum(accuracy_arr)/len(accuracy_arr)}')\n",
        "print(f'Average Cost/Loss: {sum(loss_arr)/len(loss_arr)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 95.77917585004734\n",
            "Average Cost/Loss: 0.1166059672832489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7JE7YyYveKf"
      },
      "source": [
        "**Final Sanity Check:** Make sure the our neural network's predictions match up with the actual images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OazZ1sGOveKf",
        "nbpresent": {
          "id": "3cdfe0ff-f08a-4ded-af33-c091211b22a7"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "dd4d0a49-5759-4dea-94d0-05e80c391a46"
      },
      "source": [
        "# Pick a random test sample and see what the model predicts!\n",
        "sample_num = ________\n",
        "\n",
        "images, labels = iter(train_loader).next()\n",
        "test_images = images.reshape(-1, 28*28).to(device)\n",
        "test_labels = labels.to(device)\n",
        "\n",
        "prediction_outputs = nn_model(test_images[sample_num])  # outputted probabilities vector\n",
        "print(\"Prediction output: \", prediction_outputs)\n",
        "\n",
        "predicted_digit = torch.argmax(prediction_outputs) # pick the class with highest probability --> final prediction\n",
        "print(\"Predicted digit: \", predicted_digit.item()) # print predicted classification\n",
        "\n",
        "# Show actual input image\n",
        "plt.imshow(images[sample_num][0], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction output:  tensor([ -0.6016,  -5.6135,  16.3111,   7.2328, -11.2424,  -6.7340,  -6.2604,\n",
            "          5.3097,   2.2134,  -5.3936], grad_fn=<AddBackward0>)\n",
            "Predicted digit:  2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0d8fm_EuveKk"
      },
      "source": [
        "# Congrats! You just completed your first deep learning model!\n",
        "\n",
        "## Some questions to think about:\n",
        "### 1. How to initialize weights and biases for NN for an efficient training process?\n",
        "  1. Too large / small initializations may lead to exploding / vanishing gradient problem\n",
        "  2. Rules of thumb:\n",
        "    1. The mean of the activations should be zero.\n",
        "    2. The variance of the activations should stay the same across every layer.\n",
        "  3. See the [article](https://www.deeplearning.ai/ai-notes/initialization) for more detailed justification for Xavier Initialization\n",
        "\n",
        "### 2. How many layers should we have for hidden layers?\n",
        "- If your data is linearly separable, then no need to have hidden layers.\n",
        "- In most cases, it is not. Since MNIST is a small and simple image, usually having 2 hidden layers is sufficient.\n",
        "- Reference table:\n",
        "\n",
        "Num of layers | Result\n",
        "\t:--------:|:-------:\n",
        "0 | Only capable of representing linear separable functions or decisions\n",
        "1 | Can approximate any function that contains a continuous mapping\n",
        "from one finite space to another.\n",
        "2 | Can represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy.\n",
        "> 2 | Additional layers can learn complex representations (sort of automatic feature engineering) for layer layers.\n",
        "\n",
        "## 3. How to decide number of neurons?\n",
        "1. Too *few* neurons: Unable to adequately detect signals in a complicated data set, causing underfitting\n",
        "2. Too *many* neurons:\n",
        "  1. May cause overfitting: reflect information from training dataset very well but have poor capability of making predictions on unseen data\n",
        "  2. Also increases runtime & makes the neural network more difficult to train\n",
        "3. General rule of thumb:\n",
        "  1. The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
        "  2. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
        "  3. The number of hidden neurons should be less than twice the size of the input layer.\n",
        "3. Even more optimal: [apply optimization](https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9) such as a pruning algorithm to remove redundant neurons / prune weights\n",
        "  1. Removing a weight that is setting output of a neuron to 0\n",
        "  2. Use activation values as criteria: remove neurons that always output 0 (i.e. have little impact on the network)\n",
        "  3. Based on parameters of a neuron: if 2 neurons have very similar weights & biases, remove 1 while preserve functionality\n",
        "\n",
        "## 4. How to choose an activation function?\n",
        "1. Sigmoid: 0 < output < 1\n",
        "  - Used if need to output probabilities\n",
        "  - Can cause the vanishing gradient problem\n",
        "2. Tanh: -1 <= output <= 1\n",
        "  - Allow positive & negative outputs → more efficient weight updating\n",
        "  - Alleviate vanishing gradient to some extent but still cannot completely overcome it\n",
        "3. Relu:\n",
        "  - Most popular one used for hidden layers, as it overcomes vanishing gradients for backprop.\n",
        "  - However, some gradients can die if activation < 0 → Leaky ReLU\n",
        "4. Softmax:\n",
        "  - Calculate probabilities distribution of 1 event over n different events\n",
        "  - Used if our NN model is handling multiple classes problem, and it’s used in the last output layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Additional Resources:\n",
        "* CAIS++ [Blog](http://caisplusplus.usc.edu/blog/curriculum/lesson4) Posts\n",
        "* [3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk) Videos\n",
        "* ML Cheatsheet for [Activation Functions](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
        "* ML Cheatsheet for [Cost/Loss Functions](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
        "* Michael Nielsen [Free Online Book](http://neuralnetworksanddeeplearning.com)"
      ]
    }
  ]
}